---
title: "Assignment 03"
subtitle: "Hyperparameter Tunning - Group 8"
date: 10/23/2023
date-modified: last-modified
date-format: long
format:
  html:
    theme: [cosmo, theme.scss]
    toc: true
    embed-resources: true
    number-sections: true
author:
  - name: Ahmed Khair
    affiliations:
      - id: afk46
        name: Georgetown University
        city: Washington
        state: DC
  - name: , Jiazu Zhang 
    affiliations:
      - id: jz944
  - name: Mengjia Wei
    affiliations:
      - id: mw1296

---


# Instructions (Remove the instructions before submission)

This assignment will deal with tuning the hyperparameters for the [online shopping dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset). Make sure to remove the instructions and only keep Q6 onward. The qmd file of this assignment is located in the [files folder](https://georgetown.instructure.com/files/11681026/download?download_frd=1).

- This is a group assignment with independent submission on Canvas. Collaboration is essential. Use Git for version control.
- Begin by setting your random seed as the last four digits of your GUID.
- Prefix each variable with 'g#groupnumber' (e.g., g01_variableName) to ensure uniqueness and to demonstrate originality in your group's work.
- add the names of all group members to the YAML header above.
- Use of Generative AI tools, including but not restricted to GPT-3 is strictly prohibited.

## Git Commit and Collaboration

- This is a group assignment. Collaboration is essential. Use Git for version control.
- Regular and meaningful commit messages are expected, indicating steady progress and contributions from all group members.
- Avoid large, infrequent commits. Instead, aim for more minor, frequent updates showing your code's evolution and thoughts.
- Collaboration tools, especially Git, should be used as a backup tool and a truly collaborative platform. Discuss, review, and merge each other's contributions.

# Grading Criteria

- The assignment is worth 75 points.
- There are three grading milestones in the assignment.
  - Adherence to Requirements, Coding Standards, Documentation, Runtime, and Efficiency (22 Points)
    - Adherence to Requirements (5 Points): Ensure all the given requirements of the assignment, including Git commits and collaboration, are met.
    - Coding Standards (5 Points): Code should be readable and maintainable. Ensure appropriate variable naming and code commenting.
    - Documentation (6 Points): Provide explanations or reasoning for using a particular command and describe the outputs. Avoid vague descriptions; aim for clarity and depth.
    - Runtime (3 Points): The code should execute without errors and handle possible exceptions.
    - Efficiency (3 Points): Implement efficient coding practices, avoid redundancy, and optimize for performance where applicable.
  - Collaborative Programming (13 Points)
    - GitHub Repository Structure (3 Points): A well-organized repository with clear directory structures and meaningful file names.
    - Number of Commits (3 Points): Reflects steady progress and contributions from all group members.
    - Commit Quality (3 Points): Clear, descriptive commit messages representing logical chunks of work. Avoid trivial commits like "typo fix."
    - Collaboration & Contribution (4 Points): Demonstrated teamwork where each member contributes significantly. This can be seen through pull requests, code reviews, and merge activities.
  - Assignment Questions (40 Points)

# Adherence to Requirements, Coding Standards, Documentation, Runtime, and Efficiency (22 Points)
This section is graded based on adherence to Requirements, Coding Standards, 
Documentation, Runtime, and Efficiency.

# Collaborative Programming (13 Points)

This section is graded based on the Github submission. Each person needs to have made commits to the repository. GitHub Repository Structure, Number of Commits, Commit Quality, Collaboration, and Contribution are generally graded based on the group's overall performance. However, if there is a significant difference in the number of commits or contributions between group members, the instructor may adjust the grade accordingly.


# Assignment Questions (40 Points)

# Data Preparation (7 Points):

```{python}
# Loading the necessary packages
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import random
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import metrics 
import xgboost as xgb
from time import strptime

# set random seed
random.seed(1296)


```

## Load the dataset and display the dataframe (2 Points).

```{python}
# Load the csv file
gr8_osi = pd.read_csv("datasets/online_shoppers_intention.csv", header=0)
# Display the dataframe
gr8_osi
```

## Use `describe` to provide statistics on the pandas Datafra me (2 Points).

```{python}
gr8_osi.describe()

```

## Split the dataset into a Training set and a Test set. Justify your preferred split (3 Points).

```{python}
#drop na values from whole dataset
gr8_osi = gr8_osi.dropna()
# reformat categorfical columns to integers
gr8_osi['Month'] = gr8_osi['Month'].str.replace('June','Jun')
gr8_osi['Month'] = pd.to_datetime(gr8_osi.Month, format='%b').dt.month.astype(str)
gr8_osi["Weekend"] = gr8_osi["Weekend"].astype(int)
gr8_osi["Revenue"] = gr8_osi["Revenue"].astype(int)
gr8_osi['VisitorType'] = np.select([gr8_osi.VisitorType =='Returning_Visitor', gr8_osi.VisitorType=='New_Visitor',gr8_osi.VisitorType=='Other'], [0,1,2], default=None)

#check data type again
gr8_osi.info()

#convert TrafficType column to integer
gr8_osi['VisitorType'].astype(int)

# split train and test dataset
# In this step, we decide to keep 70% of the data for training for and 30% for testing
gr8_osi_X = gr8_osi.loc[:, gr8_osi.columns != 'Revenue'].to_numpy()
gr8_osi_y = gr8_osi.loc[:, gr8_osi.columns == 'Revenue'].to_numpy()
gr8_osiX_train, gr8_osiX_test, gr8_osiy_train, gr8_osiy_test = train_test_split(
    gr8_osi_X, gr8_osi_y, test_size=0.30, random_state=0)

```

# Classification Routine (12 Points):

Execute a classification routine using RandomForestClassifier(), BaggingClassifier(), and XGboostclassifier(). Independently output the accuracy box plot as discussed in class. Use any package you are comfortable with (seaborn, matplotlib).

## RandomForestClassifier():

```{python}
# Add code here

```

## BaggingClassifier():

```{python}
# Add code here

```

## XGboostclassifier():

```{python}
# Add code here

```

# Classification with GridSearchCV (8 Points):

Replicate the classification from Q2 using GridsearchCV().
### Random Forest Classification with GridSearchCV:
```{python}
# split train and test dataset
# In this step, we decide to keep 70% of the data for training for and 30% for testing
gr8_osi_X = gr8_osi.loc[:, gr8_osi.columns != 'Revenue'].to_numpy()
gr8_osi_y = gr8_osi.loc[:, gr8_osi.columns == 'Revenue'].to_numpy()
gr8_osiX_train, gr8_osiX_test, gr8_osiy_train, gr8_osiy_test = train_test_split(
    gr8_osi_X, gr8_osi_y, test_size=0.30, random_state=0)

# Defining hyperparameter space with max_depth, min_sample_split
hyperparameter_grid = {'max_depth':[2,4,6,8,10], 
                      'n_estimators': np.arange(5,30,5)}
gs_rfc = GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=hyperparameter_grid, n_jobs=-1, cv=5)
gs_rfc.fit(gr8_osiX_train, gr8_osiy_train)

# display gridsearch results in dataframe
gs_rfc_result=pd.DataFrame(index=range(0,len(gs_rfc.cv_results_['mean_score_time'])),columns=['param_max_depth','param_n_estimators', 'mean_test_score', 'std_test_score'])
gs_rfc.cv_results_.keys()
for i in ['param_max_depth','param_n_estimators', 'mean_test_score', 'std_test_score']:
    gs_rfc_result[i]=gs_rfc.cv_results_[i]
```
### Bagging Classification with GridSearchCV:
```{python}

```

### XG Boost Classification with GridSearchCV:
```{python}

```

# Classification with RandomSearchCV (8 Points):

Replicate the classification from Q2 using RandomSearchCV().

```{python}
# Add code here

```

# Comparison and Analysis (5 Points):

Compare the results from Q2, Q3, and Q4. Describe the best hyperparameters for all three experiments.