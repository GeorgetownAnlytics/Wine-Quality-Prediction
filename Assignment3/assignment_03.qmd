---
title: "Assignment 03"
subtitle: "Hyperparameter Tunning - Group 8"
date: 10/23/2023
date-modified: last-modified
date-format: long
format:
  html:
    theme: [cosmo, theme.scss]
    toc: true
    embed-resources: true
    number-sections: true
author:
  - name: Ahmed Khair
    affiliations:
      - id: afk46
        name: Georgetown University
        city: Washington
        state: DC
  - name: , Jiazu Zhang 
    affiliations:
      - id: jz944
  - name: Mengjia Wei
    affiliations:
      - id: mw1296

---


# Data Preparation (7 Points):

```{python}
# Loading the necessary packages
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import random
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import metrics 
import xgboost as xgb
from xgboost import XGBClassifier
from time import strptime
import warnings
warnings.filterwarnings('ignore')
# set random seed
random.seed(1296)
```

## Load the dataset and display the dataframe (2 Points).

```{python}
# Load the csv file
gr8_osi = pd.read_csv("datasets/online_shoppers_intention.csv", header=0)
# Display the dataframe
gr8_osi
```

## Use `describe` to provide statistics on the pandas Datafra me (2 Points).

```{python}
gr8_osi.describe()

```

## Split the dataset into a Training set and a Test set. Justify your preferred split (3 Points).

```{python}
#drop na values from whole dataset
gr8_osi = gr8_osi.dropna()
# reformat categorfical columns to integers
gr8_osi['Month'] = gr8_osi['Month'].str.replace('June','Jun')
gr8_osi['Month'] = pd.to_datetime(gr8_osi.Month, format='%b').dt.month.astype(str)
gr8_osi["Weekend"] = gr8_osi["Weekend"].astype(int)
gr8_osi["Revenue"] = gr8_osi["Revenue"].astype(int)
gr8_osi['VisitorType'] = np.select([gr8_osi.VisitorType =='Returning_Visitor', gr8_osi.VisitorType=='New_Visitor',gr8_osi.VisitorType=='Other'], [0,1,2], default=None)

#check data type again
gr8_osi.info()

#convert TrafficType column to integer
gr8_osi['VisitorType'].astype(int)

# split train and test dataset
# In this step, we decide to keep 70% of the data for training for and 30% for testing
gr8_osi_X = gr8_osi.loc[:, gr8_osi.columns != 'Revenue'].to_numpy()
gr8_osi_y = gr8_osi.loc[:, gr8_osi.columns == 'Revenue'].to_numpy()
gr8_osiX_train, gr8_osiX_test, gr8_osiy_train, gr8_osiy_test = train_test_split(
    gr8_osi_X, gr8_osi_y, test_size=0.30, random_state=0)

```

# Classification Routine (12 Points):

Execute a classification routine using RandomForestClassifier(), BaggingClassifier(), and XGboostclassifier(). Independently output the accuracy box plot as discussed in class. Use any package you are comfortable with (seaborn, matplotlib).

## RandomForestClassifier():

```{python}
# Add code here

```

## BaggingClassifier():

```{python}
# Add code here

```

## XGboostclassifier():

```{python}
# Add code here

```

# Classification with GridSearchCV (8 Points):

Replicate the classification from Q2 using GridsearchCV().
```{python}
# split train and test dataset
# In this step, we decide to keep 70% of the data for training for and 30% for testing
gr8_osi_X = gr8_osi.loc[:, gr8_osi.columns != 'Revenue'].to_numpy()
gr8_osi_y = gr8_osi.loc[:, gr8_osi.columns == 'Revenue'].to_numpy()
gr8_osiX_train, gr8_osiX_test, gr8_osiy_train, gr8_osiy_test = train_test_split(
    gr8_osi_X, gr8_osi_y, test_size=0.30, random_state=0)
```

### Random Forest Classification with GridSearchCV:
```{python}
# Defining hyperparameter space with max_depth, n_estimators
hyperparameter_grid = {'max_depth':[2,4,6,8,10], 
                      'n_estimators': [5,10,50,100,200,500,1000]}

# Create GridsSearch model with Random Forest
gs_rfc = GridSearchCV(
  estimator=RandomForestClassifier(random_state=0),
   param_grid=hyperparameter_grid, 
   n_jobs=-1, 
   cv=5)

# Fit GridSearch model
gs_rfc.fit(gr8_osiX_train, gr8_osiy_train)

# display gridsearch results in dataframe
gs_rfc_result=pd.DataFrame(index=range(0,len(gs_rfc.cv_results_['mean_score_time'])),columns=['param_max_depth','param_n_estimators', 'mean_test_score', 'std_test_score'])
gs_rfc.cv_results_.keys()
for i in ['param_max_depth','param_n_estimators', 'mean_test_score', 'std_test_score']:
    gs_rfc_result[i]=gs_rfc.cv_results_[i]

gs_rfc_result.sort_values(by='mean_test_score',ascending= False)
```

### Bagging Classification with GridSearchCV:
```{python}
# Defining hyperparameter space with n_estimators, max_features, boostrap
hyperparameter_grid_bg = {
  'n_estimators': [5,10,50,100,200,500,1000],
  #'n_estimators': [5,100],
  'max_features': [0.1, 0.2, 0.5, 0.8, 0.90, 1.0],
  #'max_features': [0.1, 0.90],
  'bootstrap': [True, False]
}

# Create GridsSearch model with Bagging
gs_bgc = GridSearchCV(
  estimator=BaggingClassifier(),
  param_grid=hyperparameter_grid_bg,
  n_jobs=-1,
  cv=5
)

# Fit the GridSearch model
gs_bgc.fit(gr8_osiX_train, gr8_osiy_train)

# Display Gridsearch result in dataframe
gs_bgc_result=pd.DataFrame(gs_bgc.cv_results_)[['param_max_features','param_n_estimators', 'param_bootstrap', 'mean_test_score', 'std_test_score']]
gs_bgc_result.sort_values(by='mean_test_score', ascending=False)

```

### XG Boost Classification with GridSearchCV:
```{python}
# Defining hyperparameter space with n_estimators, max_features, boostrap
hyperparameter_grid_xgb = {
  'min_child_weight':[6,10,12],
  'max_depth': [2,3,4,5,6],
  'eta': [0.1,0.3,0.5,0.8,0.9],
  'gamma': [0,0.1,0.3,0.5,1,5,10,100,1000],
  'max_depth' : [0,1,2,3,4,5,6,10]
}

# Create GridsSearch model with xgboost
gs_xgb = GridSearchCV(
  estimator=XGBClassifier(),
  param_grid=hyperparameter_grid_xgb,
  n_jobs=-1,
  cv=5
)

# Fit the GridSearch model
gs_xgb.fit(gr8_osiX_train, gr8_osiy_train)

# Display Gridsearch result in dataframe
gs_xgb_result=pd.DataFrame(gs_xgb.cv_results_)[['param_gamma','param_min_child_weight', 'param_max_depth', 'mean_test_score', 'std_test_score']]
gs_xgb_result.sort_values(by='mean_test_score', ascending=False)
```

# Classification with RandomSearchCV (8 Points):

Replicate the classification from Q2 using RandomSearchCV().

### Random Forest Classification with RandomSearchCV()
```{python}
# Defining hyperparameter space with max_depth, n_estimators
distributions = {
    'max_depth': [10, 20, 30, 40, 50, None, 100],
    'n_estimators': [1,5,10, 50, 100, 200,1000]
}

randomforest = RandomForestClassifier(random_state=0)

rs_rfc = RandomizedSearchCV(
    estimator=randomforest,
    param_distributions=distributions,
    random_state=0,
    n_iter=10,
    cv=5 
)

rs_rfc.fit(gr8_osiX_train, gr8_osiy_train)

# display search results in dataframe



rs_rfc_result=pd.DataFrame(index=range(0,len(rs_rfc.cv_results_['mean_score_time'])),columns=['param_max_depth', 'param_n_estimators', 'mean_test_score', 'std_test_score'])
rs_rfc.cv_results_.keys()
for i in ['param_max_depth', 'param_n_estimators', 'mean_test_score', 'std_test_score']:
    rs_rfc_result[i]=rs_rfc.cv_results_[i]

rs_rfc_result.sort_values(by='mean_test_score',ascending= False)
```

### Bagging Classification with RandomSearchCV()
```{python}
# Defining hyperparameter space with random_state, n_estimators,max_samples
distributions = {
    'random_state':[None, 1,2],
    'n_estimators': [1,5,10,20,50,100],
    'max_samples': [1,5,10]
}

Bagging = BaggingClassifier(random_state=0)

rs_bag = RandomizedSearchCV(
    estimator=Bagging,
    param_distributions=distributions,
    random_state=0,
    n_iter=10,
    cv=5 
)

rs_bag.fit(gr8_osiX_train, gr8_osiy_train)

# display search results in dataframe


rs_bag_result=pd.DataFrame(index=range(0,len(rs_bag.cv_results_['mean_score_time'])),columns=['param_random_state','param_n_estimators','param_max_samples','mean_test_score', 'std_test_score'])
rs_bag.cv_results_.keys()
for i in ['param_random_state','param_n_estimators','param_max_samples','mean_test_score', 'std_test_score']:
    rs_bag_result[i]=rs_bag.cv_results_[i]

rs_bag_result.sort_values(by='mean_test_score',ascending= False)
```

### XGboost Classification with RandomSearchCV()
```{python}
# Defining hyperparameter space with 
distributions = {
    'device': ['cpu'],
    'eta': [0.1,0.3,0.5,0.8,0.9],
    'gamma': [0,0.1,0.3,0.5,1,5,10,100,1000],
    'max_depth' : [0,1,2,3,4,5,6,10]
}

xgboost = XGBClassifier(random_state=0)

rs_xgb = RandomizedSearchCV(
    estimator=xgboost,
    param_distributions=distributions,
    random_state=0,
    n_iter=10,
    cv=5 
)

rs_xgb.fit(gr8_osiX_train, gr8_osiy_train)

# display search results in dataframe


rs_xgb_result=pd.DataFrame(index=range(0,len(rs_xgb.cv_results_['mean_score_time'])),columns=['param_eta','param_gamma','param_max_depth','mean_test_score', 'std_test_score'])
rs_xgb.cv_results_.keys()
for i in ['param_eta','param_gamma','param_max_depth','mean_test_score', 'std_test_score']:
    rs_xgb_result[i]=rs_xgb.cv_results_[i]

rs_xgb_result.sort_values(by='mean_test_score',ascending= False)
```
```


# Comparison and Analysis (5 Points):

Compare the results from Q2, Q3, and Q4. Describe the best hyperparameters for all three experiments.